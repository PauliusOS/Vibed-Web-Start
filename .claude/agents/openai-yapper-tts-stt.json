{
  "_creationTime": 1761386435647.8706,
  "_id": "kn70tw4g08yk7jwn1zf19fwvns7t5h1x",
  "authorEmail": "0xpaulius@gmail.com",
  "authorName": "Paulius",
  "categories": [
    "AI"
  ],
  "clerkId": "user_30mlLem6xz7dBn7sqJNMUozNjBR",
  "createdAt": 1761386435648.0,
  "databases": [
    "Convex"
  ],
  "description": "Implements voice conversation features using OpenAI STT/TTS and Vercel AI SDK for Expo + Convex apps. Use when adding voice recording, transcription, AI chat, or text-to-speech to any React Native application",
  "frameworks": [
    "Expo"
  ],
  "instructions": "---\nname: agent-openai-voice\ndescription: Implements voice conversation features using OpenAI STT/TTS and Vercel AI SDK for Expo + Convex apps. Use when adding voice recording, transcription, AI chat, or text-to-speech to any React Native application.\ntools: \"*\"\nmodel: inherit\ncolor: purple\n---\n\n# Agent: OpenAI Voice Implementation\n\nImplements complete voice conversation features using OpenAI Whisper (STT), GPT models, and TTS with Vercel AI SDK for Expo React Native + Convex backends.\n\n## What This Agent Does\n\nBuilds production-ready voice conversation systems with:\n- Audio recording with permissions and time limits\n- Speech-to-text using OpenAI Whisper\n- AI responses via Vercel AI SDK (streaming or non-streaming)\n- Text-to-speech using OpenAI TTS\n- Audio playback with controls\n- Conversation history persistence\n\n## Implementation Workflow\n\n### Phase 1: Dependencies & Configuration\n\n**Install Required Packages:**\n```bash\n# Audio libraries\nnpx expo install expo-audio expo-file-system\n\n# AI SDKs\nnpm install ai @ai-sdk/openai openai\n\n# UI/Animation (optional)\nnpm install lottie-react-native expo-linear-gradient\n```\n\n**Environment Variables:**\n```bash\n# Add to Convex environment (never in frontend .env)\nnpx convex env set OPENAI_API_KEY sk-proj-your-key-here\n```\n\n**Audio Permissions (app.json):**\n```json\n{\n  \"expo\": {\n    \"plugins\": [\n      [\n        \"expo-audio\",\n        {\n          \"microphonePermission\": \"Allow $(PRODUCT_NAME) to access your microphone for voice conversations.\"\n        }\n      ]\n    ]\n  }\n}\n```\n\n---\n\n### Phase 2: Convex Backend Functions\n\n#### Schema Definition\n\n**File:** `convex/schema.ts`\n\n```typescript\nimport { defineSchema, defineTable } from \"convex/server\";\nimport { v } from \"convex/values\";\n\nexport default defineSchema({\n  // ... existing tables\n\n  voiceConversations: defineTable({\n    userId: v.string(),\n    transcription: v.string(),          // User's spoken input\n    aiResponse: v.string(),              // AI's text response\n    audioResponseUrl: v.optional(v.string()), // Optional TTS audio URL\n    contextData: v.optional(v.string()), // Optional app-specific context\n    createdAt: v.number(),\n  })\n    .index(\"by_user\", [\"userId\"])\n    .index(\"by_created_at\", [\"createdAt\"]),\n});\n```\n\n#### Voice Processing Actions\n\n**File:** `convex/voice.ts`\n\n```typescript\nimport { v } from \"convex/values\";\nimport { action, internalAction, internalMutation } from \"./_generated/server\";\nimport { internal } from \"./_generated/api\";\nimport OpenAI from \"openai\";\nimport { streamText } from \"ai\";\nimport { openai as openaiProvider } from \"@ai-sdk/openai\";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n});\n\n// Action 1: Speech-to-Text using Whisper\nexport const transcribeAudio = action({\n  args: {\n    audioBase64: v.string(),\n    format: v.string(), // 'mp3', 'wav', 'mp4' (m4a)\n  },\n  handler: async (ctx, args): Promise<{ text: string }> => {\n    // Convert base64 to buffer\n    const audioBuffer = Buffer.from(args.audioBase64, \"base64\");\n\n    // Create File object for Whisper API\n    const file = new File([audioBuffer], `recording.${args.format}`, {\n      type: `audio/${args.format}`,\n    });\n\n    // Transcribe using Whisper\n    const transcription = await openai.audio.transcriptions.create({\n      file: file,\n      model: \"whisper-1\",\n      language: \"en\", // Change as needed or omit for auto-detection\n      response_format: \"json\",\n    });\n\n    return { text: transcription.text };\n  },\n});\n\n// Action 2: AI Response Generation (Streaming)\nexport const generateAIResponse = action({\n  args: {\n    userMessage: v.string(),\n    systemPrompt: v.string(),\n    contextData: v.optional(v.string()),\n    modelName: v.optional(v.string()),\n    maxTokens: v.optional(v.number()),\n  },\n  handler: async (ctx, args): Promise<{ response: string }> => {\n    const model = args.modelName || \"gpt-4o\"; // gpt-4o, gpt-4o-mini, etc.\n\n    // Build system message with context\n    const systemMessage = args.contextData\n      ? `${args.systemPrompt}\\n\\nContext: ${args.contextData}`\n      : args.systemPrompt;\n\n    // Use Vercel AI SDK for streaming\n    const result = await streamText({\n      model: openaiProvider(model),\n      system: systemMessage,\n      prompt: args.userMessage,\n      maxTokens: args.maxTokens || 500,\n    });\n\n    // Collect full response\n    let fullResponse = \"\";\n    for await (const textPart of result.textStream) {\n      fullResponse += textPart;\n    }\n\n    return { response: fullResponse };\n  },\n});\n\n// Action 3: Text-to-Speech using OpenAI TTS\nexport const generateSpeech = action({\n  args: {\n    text: v.string(),\n    voice: v.optional(v.string()),\n    model: v.optional(v.string()),\n  },\n  handler: async (ctx, args): Promise<{ audioBase64: string; format: string }> => {\n    const voice = args.voice || \"nova\"; // alloy, echo, fable, onyx, nova, shimmer\n    const model = args.model || \"tts-1\"; // tts-1 or tts-1-hd\n\n    const speechResponse = await openai.audio.speech.create({\n      model: model as \"tts-1\" | \"tts-1-hd\",\n      voice: voice as any,\n      input: args.text,\n      response_format: \"mp3\",\n      speed: 1.0,\n    });\n\n    // Convert to base64 for transfer\n    const buffer = Buffer.from(await speechResponse.arrayBuffer());\n    const base64Audio = buffer.toString(\"base64\");\n\n    return { audioBase64: base64Audio, format: \"mp3\" };\n  },\n});\n\n// Action 4: Complete Voice Conversation Workflow\nexport const processVoiceConversation = action({\n  args: {\n    userId: v.string(),\n    audioBase64: v.string(),\n    audioFormat: v.string(),\n    systemPrompt: v.string(),\n    contextData: v.optional(v.string()),\n    voicePreference: v.optional(v.string()),\n  },\n  handler: async (ctx, args): Promise<{\n    transcription: string;\n    response: string;\n    audioBase64: string;\n  }> => {\n    // Step 1: Transcribe audio\n    const transcription = await ctx.runAction(internal.voice.transcribeAudio, {\n      audioBase64: args.audioBase64,\n      format: args.audioFormat,\n    });\n\n    // Step 2: Generate AI response\n    const aiResponse = await ctx.runAction(internal.voice.generateAIResponse, {\n      userMessage: transcription.text,\n      systemPrompt: args.systemPrompt,\n      contextData: args.contextData,\n    });\n\n    // Step 3: Convert to speech\n    const speech = await ctx.runAction(internal.voice.generateSpeech, {\n      text: aiResponse.response,\n      voice: args.voicePreference,\n    });\n\n    // Step 4: Save conversation\n    await ctx.runMutation(internal.voice.saveConversation, {\n      userId: args.userId,\n      transcription: transcription.text,\n      aiResponse: aiResponse.response,\n      contextData: args.contextData,\n    });\n\n    return {\n      transcription: transcription.text,\n      response: aiResponse.response,\n      audioBase64: speech.audioBase64,\n    };\n  },\n});\n\n// Internal Mutation: Save Conversation\nexport const saveConversation = internalMutation({\n  args: {\n    userId: v.string(),\n    transcription: v.string(),\n    aiResponse: v.string(),\n    contextData: v.optional(v.string()),\n  },\n  handler: async (ctx, args) => {\n    return await ctx.db.insert(\"voiceConversations\", {\n      userId: args.userId,\n      transcription: args.transcription,\n      aiResponse: args.aiResponse,\n      contextData: args.contextData,\n      createdAt: Date.now(),\n    });\n  },\n});\n\n// Query: Get Conversation History\nexport const getConversationHistory = internalAction({\n  args: {\n    userId: v.string(),\n    limit: v.optional(v.number()),\n  },\n  handler: async (ctx, args) => {\n    const conversations = await ctx.runQuery(internal.voice.getConversations, {\n      userId: args.userId,\n      limit: args.limit || 10,\n    });\n    return conversations;\n  },\n});\n```\n\n---\n\n### Phase 3: React Native Hooks\n\n#### Hook 1: Audio Recording\n\n**File:** `hooks/useVoiceRecording.ts`\n\n```typescript\nimport { useState, useEffect } from \"react\";\nimport { Audio, AudioModule } from \"expo-audio\";\n\nexport const useVoiceRecording = (maxDurationMs: number = 300000) => {\n  const [recording, setRecording] = useState<Audio.Recording | null>(null);\n  const [isRecording, setIsRecording] = useState(false);\n  const [durationMs, setDurationMs] = useState(0);\n  const [hasPermission, setHasPermission] = useState<boolean | null>(null);\n\n  const isMaxDuration = durationMs >= maxDurationMs;\n\n  // Request permissions on mount\n  useEffect(() => {\n    (async () => {\n      const { granted } = await Audio.requestPermissionsAsync();\n      setHasPermission(granted);\n    })();\n  }, []);\n\n  // Update duration during recording\n  useEffect(() => {\n    let interval: NodeJS.Timeout;\n    if (isRecording && recording) {\n      interval = setInterval(async () => {\n        const status = await recording.getStatusAsync();\n        if (status.isRecording) {\n          setDurationMs(status.durationMillis);\n        }\n      }, 100);\n    }\n    return () => {\n      if (interval) clearInterval(interval);\n    };\n  }, [isRecording, recording]);\n\n  const startVoiceRecording = async () => {\n    if (!hasPermission) {\n      const { granted } = await Audio.requestPermissionsAsync();\n      if (!granted) {\n        throw new Error(\"Microphone permission denied\");\n      }\n      setHasPermission(true);\n    }\n\n    try {\n      // Set audio mode for recording\n      await AudioModule.setAudioModeAsync({\n        allowsRecording: true,\n        playsInSilentMode: true,\n      });\n\n      // Start recording\n      const { recording: newRecording } = await Audio.Recording.createAsync(\n        Audio.RecordingPresets.HIGH_QUALITY\n      );\n\n      setRecording(newRecording);\n      setIsRecording(true);\n      setDurationMs(0);\n    } catch (error) {\n      console.error(\"Failed to start recording:\", error);\n      throw error;\n    }\n  };\n\n  const stopVoiceRecording = async (): Promise<{ fileUri: string }> => {\n    if (!recording) {\n      throw new Error(\"No active recording\");\n    }\n\n    try {\n      await recording.stopAndUnloadAsync();\n      const uri = recording.getURI();\n      setIsRecording(false);\n      setRecording(null);\n\n      if (!uri) {\n        throw new Error(\"Recording URI is null\");\n      }\n\n      return { fileUri: uri };\n    } catch (error) {\n      console.error(\"Failed to stop recording:\", error);\n      throw error;\n    }\n  };\n\n  const reset = () => {\n    setRecording(null);\n    setIsRecording(false);\n    setDurationMs(0);\n  };\n\n  return {\n    startVoiceRecording,\n    stopVoiceRecording,\n    isRecording,\n    durationMs,\n    isMaxDuration,\n    hasPermission,\n    reset,\n  };\n};\n```\n\n#### Hook 2: Voice Processing\n\n**File:** `hooks/useVoiceProcessing.ts`\n\n```typescript\nimport { useState } from \"react\";\nimport { useAction } from \"convex/react\";\nimport { api } from \"@/convex/_generated/api\";\nimport * as FileSystem from \"expo-file-system\";\n\nexport const useVoiceProcessing = () => {\n  const [isProcessing, setIsProcessing] = useState(false);\n  const [transcription, setTranscription] = useState(\"\");\n  const [response, setResponse] = useState(\"\");\n  const [audioBase64, setAudioBase64] = useState<string | null>(null);\n  const [error, setError] = useState<string | null>(null);\n\n  const processConversation = useAction(api.voice.processVoiceConversation);\n\n  const submitRecording = async (\n    audioUri: string,\n    userId: string,\n    systemPrompt: string,\n    contextData?: string,\n    voicePreference?: string\n  ) => {\n    setIsProcessing(true);\n    setError(null);\n\n    try {\n      // Auto-detect audio format from file extension\n      let format = \"mp4\"; // Default for .m4a\n      if (audioUri.toLowerCase().endsWith(\".mp3\")) format = \"mp3\";\n      else if (audioUri.toLowerCase().endsWith(\".wav\")) format = \"wav\";\n      else if (audioUri.toLowerCase().endsWith(\".m4a\")) format = \"mp4\";\n\n      // Read audio file and convert to base64\n      const audioBase64 = await FileSystem.readAsStringAsync(audioUri, {\n        encoding: FileSystem.EncodingType.Base64,\n      });\n\n      // Process through Convex\n      const result = await processConversation({\n        userId,\n        audioBase64,\n        audioFormat: format,\n        systemPrompt,\n        contextData,\n        voicePreference,\n      });\n\n      setTranscription(result.transcription);\n      setResponse(result.response);\n      setAudioBase64(result.audioBase64);\n    } catch (err: any) {\n      console.error(\"Error processing voice conversation:\", err);\n      setError(err.message || \"Failed to process voice conversation\");\n      throw err;\n    } finally {\n      setIsProcessing(false);\n    }\n  };\n\n  const reset = () => {\n    setTranscription(\"\");\n    setResponse(\"\");\n    setAudioBase64(null);\n    setError(null);\n  };\n\n  return {\n    submitRecording,\n    isProcessing,\n    transcription,\n    response,\n    audioBase64,\n    error,\n    reset,\n  };\n};\n```\n\n#### Hook 3: Audio Playback\n\n**File:** `hooks/useAudioPlayback.ts`\n\n```typescript\nimport { useState, useEffect } from \"react\";\nimport { useAudioPlayer, AudioModule } from \"expo-audio\";\n\nexport const useAudioPlayback = () => {\n  const player = useAudioPlayer();\n  const [isPlaying, setIsPlaying] = useState(false);\n  const [isLoading, setIsLoading] = useState(false);\n\n  // Monitor playback status\n  useEffect(() => {\n    if (player) {\n      setIsPlaying(player.playing);\n    }\n  }, [player.playing]);\n\n  const playAudioFromBase64 = async (base64: string) => {\n    try {\n      setIsLoading(true);\n\n      // Set audio mode for playback\n      await AudioModule.setAudioModeAsync({\n        allowsRecording: false,\n        playsInSilentMode: true,\n      });\n\n      // Convert base64 to data URI\n      const audioUri = `data:audio/mp3;base64,${base64}`;\n\n      // Replace source and play\n      player.replace({ uri: audioUri });\n      player.play();\n\n      setIsPlaying(true);\n    } catch (error) {\n      console.error(\"Error playing audio:\", error);\n      throw error;\n    } finally {\n      setIsLoading(false);\n    }\n  };\n\n  const pauseAudio = () => {\n    if (player) {\n      player.pause();\n      setIsPlaying(false);\n    }\n  };\n\n  const resumeAudio = () => {\n    if (player) {\n      player.play();\n      setIsPlaying(true);\n    }\n  };\n\n  const stopAudio = () => {\n    if (player) {\n      player.pause();\n      player.seekTo(0);\n      setIsPlaying(false);\n    }\n  };\n\n  return {\n    playAudioFromBase64,\n    pauseAudio,\n    resumeAudio,\n    stopAudio,\n    isPlaying,\n    isLoading,\n  };\n};\n```\n\n---\n\n### Phase 4: UI Component\n\n**File:** `components/VoiceInterface.tsx`\n\n```typescript\nimport React, { useEffect } from \"react\";\nimport { View, Text, TouchableOpacity, StyleSheet, Alert } from \"react-native\";\nimport { Mic, StopCircle, Play, Pause } from \"lucide-react-native\";\nimport { useVoiceRecording } from \"@/hooks/useVoiceRecording\";\nimport { useVoiceProcessing } from \"@/hooks/useVoiceProcessing\";\nimport { useAudioPlayback } from \"@/hooks/useAudioPlayback\";\nimport { useAuth } from \"@clerk/clerk-expo\";\n\ninterface VoiceInterfaceProps {\n  systemPrompt: string;\n  contextData?: string;\n  voicePreference?: string;\n  maxRecordingSeconds?: number;\n  onTranscriptionComplete?: (text: string) => void;\n  onResponseComplete?: (text: string) => void;\n}\n\nexport const VoiceInterface: React.FC<VoiceInterfaceProps> = ({\n  systemPrompt,\n  contextData,\n  voicePreference = \"nova\",\n  maxRecordingSeconds = 300,\n  onTranscriptionComplete,\n  onResponseComplete,\n}) => {\n  const { userId } = useAuth();\n\n  const {\n    startVoiceRecording,\n    stopVoiceRecording,\n    isRecording,\n    durationMs,\n    isMaxDuration,\n    hasPermission,\n  } = useVoiceRecording(maxRecordingSeconds * 1000);\n\n  const {\n    submitRecording,\n    isProcessing,\n    transcription,\n    response,\n    audioBase64,\n    error,\n  } = useVoiceProcessing();\n\n  const {\n    playAudioFromBase64,\n    pauseAudio,\n    resumeAudio,\n    isPlaying,\n  } = useAudioPlayback();\n\n  // Auto-stop at max duration\n  useEffect(() => {\n    if (isMaxDuration && isRecording) {\n      handleStopRecording();\n    }\n  }, [isMaxDuration]);\n\n  // Auto-play response when ready\n  useEffect(() => {\n    if (audioBase64) {\n      playAudioFromBase64(audioBase64);\n    }\n  }, [audioBase64]);\n\n  // Callbacks\n  useEffect(() => {\n    if (transcription && onTranscriptionComplete) {\n      onTranscriptionComplete(transcription);\n    }\n  }, [transcription]);\n\n  useEffect(() => {\n    if (response && onResponseComplete) {\n      onResponseComplete(response);\n    }\n  }, [response]);\n\n  const handleStartRecording = async () => {\n    if (!hasPermission) {\n      Alert.alert(\"Permission Required\", \"Microphone access is required for voice recording.\");\n      return;\n    }\n    try {\n      await startVoiceRecording();\n    } catch (err: any) {\n      Alert.alert(\"Recording Error\", err.message);\n    }\n  };\n\n  const handleStopRecording = async () => {\n    if (!userId) {\n      Alert.alert(\"Authentication Error\", \"You must be signed in to use voice features.\");\n      return;\n    }\n\n    try {\n      const recording = await stopVoiceRecording();\n      await submitRecording(\n        recording.fileUri,\n        userId,\n        systemPrompt,\n        contextData,\n        voicePreference\n      );\n    } catch (err: any) {\n      Alert.alert(\"Processing Error\", err.message || \"Failed to process recording\");\n    }\n  };\n\n  const formatTime = (ms: number) => {\n    const totalSeconds = Math.floor(ms / 1000);\n    const minutes = Math.floor(totalSeconds / 60);\n    const seconds = totalSeconds % 60;\n    return `${minutes}:${seconds.toString().padStart(2, \"0\")}`;\n  };\n\n  return (\n    <View style={styles.container}>\n      <Text style={styles.title}>Voice Conversation</Text>\n\n      {/* Recording Controls */}\n      {!isRecording && !isProcessing && !response && (\n        <TouchableOpacity style={styles.recordButton} onPress={handleStartRecording}>\n          <Mic size={32} color=\"#fff\" />\n          <Text style={styles.buttonText}>Start Speaking</Text>\n        </TouchableOpacity>\n      )}\n\n      {/* Recording in Progress */}\n      {isRecording && (\n        <View style={styles.recordingContainer}>\n          <View style={styles.pulsingCircle} />\n          <Text style={styles.timer}>\n            {formatTime(durationMs)} / {formatTime(maxRecordingSeconds * 1000)}\n          </Text>\n          <TouchableOpacity style={styles.stopButton} onPress={handleStopRecording}>\n            <StopCircle size={32} color=\"#fff\" />\n            <Text style={styles.buttonText}>Stop</Text>\n          </TouchableOpacity>\n        </View>\n      )}\n\n      {/* Processing */}\n      {isProcessing && (\n        <View style={styles.processingContainer}>\n          <Text style={styles.statusText}>Processing your message...</Text>\n        </View>\n      )}\n\n      {/* Transcription Display */}\n      {transcription && (\n        <View style={styles.transcriptionContainer}>\n          <Text style={styles.label}>You said:</Text>\n          <Text style={styles.transcriptionText}>{transcription}</Text>\n        </View>\n      )}\n\n      {/* Response Display */}\n      {response && (\n        <View style={styles.responseContainer}>\n          <Text style={styles.label}>AI Response:</Text>\n          <Text style={styles.responseText}>{response}</Text>\n\n          {/* Playback Controls */}\n          <View style={styles.playbackControls}>\n            {!isPlaying ? (\n              <TouchableOpacity onPress={resumeAudio}>\n                <Play size={24} color=\"#10B981\" />\n              </TouchableOpacity>\n            ) : (\n              <TouchableOpacity onPress={pauseAudio}>\n                <Pause size={24} color=\"#10B981\" />\n              </TouchableOpacity>\n            )}\n          </View>\n        </View>\n      )}\n\n      {/* Error Display */}\n      {error && (\n        <View style={styles.errorContainer}>\n          <Text style={styles.errorText}>{error}</Text>\n        </View>\n      )}\n    </View>\n  );\n};\n\nconst styles = StyleSheet.create({\n  container: {\n    padding: 20,\n    backgroundColor: \"#1F2937\",\n    borderRadius: 12,\n    margin: 16,\n  },\n  title: {\n    fontSize: 24,\n    fontWeight: \"bold\",\n    color: \"#fff\",\n    textAlign: \"center\",\n    marginBottom: 20,\n  },\n  recordButton: {\n    backgroundColor: \"#10B981\",\n    borderRadius: 50,\n    padding: 20,\n    alignItems: \"center\",\n    justifyContent: \"center\",\n  },\n  stopButton: {\n    backgroundColor: \"#EF4444\",\n    borderRadius: 50,\n    padding: 20,\n    alignItems: \"center\",\n    justifyContent: \"center\",\n  },\n  buttonText: {\n    color: \"#fff\",\n    fontSize: 16,\n    marginTop: 8,\n  },\n  recordingContainer: {\n    alignItems: \"center\",\n    gap: 16,\n  },\n  pulsingCircle: {\n    width: 20,\n    height: 20,\n    borderRadius: 10,\n    backgroundColor: \"#EF4444\",\n  },\n  timer: {\n    fontSize: 32,\n    color: \"#fff\",\n    fontWeight: \"bold\",\n  },\n  processingContainer: {\n    padding: 20,\n    alignItems: \"center\",\n  },\n  statusText: {\n    color: \"#9CA3AF\",\n    fontSize: 16,\n    fontStyle: \"italic\",\n  },\n  transcriptionContainer: {\n    marginTop: 20,\n    padding: 16,\n    backgroundColor: \"#374151\",\n    borderRadius: 8,\n  },\n  responseContainer: {\n    marginTop: 20,\n    padding: 16,\n    backgroundColor: \"#10B981\",\n    borderRadius: 8,\n  },\n  label: {\n    fontSize: 12,\n    color: \"#9CA3AF\",\n    marginBottom: 8,\n    textTransform: \"uppercase\",\n  },\n  transcriptionText: {\n    color: \"#fff\",\n    fontSize: 14,\n    lineHeight: 20,\n  },\n  responseText: {\n    color: \"#fff\",\n    fontSize: 14,\n    lineHeight: 20,\n  },\n  playbackControls: {\n    marginTop: 12,\n    alignItems: \"center\",\n  },\n  errorContainer: {\n    marginTop: 20,\n    padding: 16,\n    backgroundColor: \"#EF4444\",\n    borderRadius: 8,\n  },\n  errorText: {\n    color: \"#fff\",\n    fontSize: 14,\n  },\n});\n```\n\n---\n\n## Usage Example\n\n**In any screen:**\n\n```typescript\nimport { VoiceInterface } from \"@/components/VoiceInterface\";\n\nexport default function ChatScreen() {\n  const systemPrompt = \"You are a helpful AI assistant. Provide concise, clear responses.\";\n  const contextData = \"User is asking about product features.\";\n\n  return (\n    <VoiceInterface\n      systemPrompt={systemPrompt}\n      contextData={contextData}\n      voicePreference=\"nova\"\n      maxRecordingSeconds={300}\n      onTranscriptionComplete={(text) => console.log(\"Transcription:\", text)}\n      onResponseComplete={(text) => console.log(\"Response:\", text)}\n    />\n  );\n}\n```\n\n---\n\n## Critical Implementation Notes\n\n### \u2705 ALWAYS Do This\n\n1. **OpenAI API Key**: ONLY in Convex environment variables, NEVER in frontend `.env`\n2. **Audio Format Detection**: Auto-detect from file extension (`.mp3`, `.wav`, `.m4a`)\n3. **Permissions**: Request microphone permissions before recording\n4. **Audio Mode Management**:\n   - Recording: `allowsRecording: true`\n   - Playback: `allowsRecording: false` (switches to loudspeaker)\n5. **User Authentication**: Always verify `userId` exists before processing\n6. **Type Safety**: Add `: Promise<ReturnType>` to all Convex action handlers\n7. **Error Handling**: Use try/catch and show user-friendly alerts\n8. **Base64 Conversion**: Use `FileSystem.readAsStringAsync()` with `Base64` encoding\n\n### \u274c NEVER Do This\n\n1. **DON'T** expose OpenAI API keys in frontend code\n2. **DON'T** use `Buffer` in Convex - not available in runtime\n3. **DON'T** store audio base64 in database - exceeds size limits\n4. **DON'T** forget to unload audio players/recorders on unmount\n5. **DON'T** hardcode model names - make them configurable\n6. **DON'T** skip permission checks - will crash on iOS\n7. **DON'T** use incorrect OpenAI model names (e.g., `gpt-5` doesn't exist yet - use `gpt-4o`)\n\n---\n\n## Model Options\n\n### Whisper STT\n- `whisper-1` - Only model available, $0.006/minute\n\n### GPT Models (via Vercel AI SDK)\n- `gpt-4o` - Most capable, multimodal\n- `gpt-4o-mini` - Faster, cheaper\n- `gpt-4-turbo` - Previous generation\n- `gpt-3.5-turbo` - Fastest, cheapest\n\n### TTS Voices\n- `alloy` - Neutral, balanced\n- `echo` - Male, warm\n- `fable` - British accent\n- `onyx` - Deep, authoritative\n- `nova` - Female, friendly (default)\n- `shimmer` - Soft, gentle\n\n---\n\n## Cost Estimation\n\n**Per 5-minute conversation:**\n- Whisper STT: $0.006/min \u00d7 5 = **$0.03**\n- GPT-4o: ~500 tokens \u00d7 $0.005/1K = **$0.0025**\n- TTS: ~200 chars \u00d7 $15/1M chars = **$0.003**\n- **Total: ~$0.035 per conversation**\n\n---\n\n## Testing Checklist\n\n- [ ] Microphone permissions requested and handled\n- [ ] Recording starts/stops correctly\n- [ ] Max duration auto-stop works\n- [ ] Audio file created with correct format\n- [ ] Whisper transcription returns accurate text\n- [ ] AI response is relevant and appropriate\n- [ ] TTS audio plays correctly\n- [ ] Playback controls work (play/pause)\n- [ ] Conversation saved to database\n- [ ] Error states handled gracefully\n- [ ] Works on both iOS and Android\n- [ ] Audio quality is acceptable\n- [ ] Response time is reasonable (< 30 seconds)\n\n---\n\n## Common Errors & Solutions\n\n### Error: \"Buffer is not defined\"\n**Solution:** Don't use `Buffer` in Convex - use `Uint8Array` or move buffer operations to client\n\n### Error: \"Recording failed to start\"\n**Solution:** Check microphone permissions and audio mode configuration\n\n### Error: \"Invalid audio format\"\n**Solution:** Verify file extension matches actual audio format (use `expo-audio` high quality preset)\n\n### Error: \"API key not found\"\n**Solution:** Set in Convex: `npx convex env set OPENAI_API_KEY sk-...`\n\n### Error: \"Model not found\"\n**Solution:** Use correct model names - `gpt-4o`, not `gpt-5`\n\n---\n\n## Advanced Features\n\n### Streaming Responses (Real-time UI Update)\n```typescript\n// In Convex action\nconst result = streamText({\n  model: openaiProvider(\"gpt-4o\"),\n  prompt: userMessage,\n});\n\n// Stream to frontend via SSE or WebSockets\nfor await (const textPart of result.textStream) {\n  // Send chunk to frontend\n  await sendChunk(textPart);\n}\n```\n\n### Conversation History\n```typescript\n// Query past conversations\nconst history = await ctx.db\n  .query(\"voiceConversations\")\n  .withIndex(\"by_user\", (q) => q.eq(\"userId\", userId))\n  .order(\"desc\")\n  .take(10);\n```\n\n### Custom Voice Selection UI\n```typescript\nconst VOICES = [\n  { id: \"alloy\", name: \"Alloy\", description: \"Neutral\" },\n  { id: \"nova\", name: \"Nova\", description: \"Friendly\" },\n  // ... more voices\n];\n\n// Let user select in settings\n```\n\n---\n\n## References\n\n- [OpenAI Whisper API](https://platform.openai.com/docs/guides/speech-to-text)\n- [OpenAI TTS API](https://platform.openai.com/docs/guides/text-to-speech)\n- [Vercel AI SDK](https://sdk.vercel.ai/docs)\n- [expo-audio Documentation](https://docs.expo.dev/versions/latest/sdk/audio/)\n- [Convex Actions](https://docs.convex.dev/functions/actions)\n",
  "isPublic": true,
  "logoStorageId": "kg20c4z88pnvxcrc40ycxxh0997t52b6",
  "logoUrl": "https://tacit-capybara-732.convex.cloud/api/storage/f9fee2c1-f605-44bf-b894-4badba6b858f",
  "mcpServers": [],
  "name": "OpenAI Yapper (TTS/STT)",
  "popularity": 3.0,
  "tags": [
    "openai",
    "tts",
    "stt",
    "GPT"
  ],
  "updatedAt": 1761386494870.0,
  "usageCount": 5.0,
  "userId": "user_30mlLem6xz7dBn7sqJNMUozNjBR"
}